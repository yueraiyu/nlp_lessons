{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson-01 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`各位同学大家好，欢迎各位开始学习我们的人工智能课程。这门课程假设大家不具备机器学习和人工智能的知识，但是希望大家具备初级的Python编程能力。根据往期同学的实际反馈，我们课程的完结之后 能力能够超过80%的计算机人工智能/深度学习方向的硕士生的能力。`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本次作业的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 复现课堂代码\n",
    "\n",
    "在本部分，你需要参照我们给大家的GitHub地址里边的课堂代码，结合课堂内容，复现内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 请回答以下问题\n",
    "\n",
    "回答以下问题，并将问题发送至 mqgao@kaikeba.com中：\n",
    "```\n",
    "    2.1. what do you want to acquire in this course？\n",
    "    2.2. what problems do you want to solve？\n",
    "    2.3. what’s the advantages you have to finish you goal?\n",
    "    2.4. what’s the disadvantages you need to overcome to finish you goal?\n",
    "    2.5. How will you plan to study in this course period?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 如何提交\n",
    "代码 + 此 jupyter 相关，提交至自己的 github 中(**所以请务必把GitHub按照班主任要求录入在Trello中**)；\n",
    "第2问，请提交至mqgao@kaikeba.com邮箱。\n",
    "#### 4. 作业截止时间\n",
    "此次作业截止时间为 2019.7.6日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 完成以下问答和编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础理论部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Can you come up out 3 sceneraies which use AI methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  1、人脸识别\n",
    "        2、语音识别+翻译\n",
    "        3、电商推荐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How do we use Github; Why do we use Jupyter and Pycharm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  github统一管理代码，方便交流和作业问题跟踪；Jupyter更方便整理知识点和验证，Pycharm IDE更方便开发。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 概率模型是随机现象的数学表示。它由样本空间，样本空间内的事件以及与每个事件相关的概率定义。描述事件发生的可能性，计算机的推理基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Can you came up with some sceneraies at which we could use Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 1、垃圾邮件过滤\n",
    "       2、电商推荐\n",
    "       3、股票、量化交易\n",
    "       4、图像、人脸识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 概率模型为计算机提供了推理凭证，AI需要处理大量随机量，这些需要概率提供数据和理论的支持。概率的计算需要大量的数据基础，数据越庞大，概率大准确性越高，概率模型的难点在于大数据的分析处理，事件的抽象以及概率的计算，好的计算方式加大量的数据支持才能保证准确性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What's the Language Model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 通过概率来表征语句是否表达正确，通过概率事件来计算一条语句出现的频率来判断语句是否合理准确。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you came up with some sceneraies at which we could use Language Model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 1、搜索引擎自动补全 2、问答机器人 3、翻译"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. What's the 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 某个词的出现仅与当前有关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. What's the disadvantages and advantages of 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 计算操作方便，准确率低"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. What't the 2-gram models;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 某个词的出现只与前面1个词相关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编程实践部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 设计你自己的句子生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何生成句子是一个很经典的问题，从1940s开始，图灵提出机器智能的时候，就使用的是人类能不能流畅和计算机进行对话。和计算机对话的一个前提是，计算机能够生成语言。\n",
    "\n",
    "计算机如何能生成语言是一个经典但是又很复杂的问题。 我们课程上为大家介绍的是一种基于规则（Rule Based）的生成方法。该方法虽然提出的时间早，但是现在依然在很多地方能够大显身手。值得说明的是，现在很多很实用的算法，都是很久之前提出的，例如，二分查找提出与1940s, Dijstra算法提出于1960s 等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在著名的电视剧，电影《西部世界》中，这些机器人们语言生成的方法就是使用的SyntaxTree生成语言的方法。\n",
    "\n",
    "> \n",
    ">\n",
    "\n",
    "![WstWorld](https://timgsa.baidu.com/timg?image&quality=80&size=b10000_10000&sec=1561818705&di=95ca9ff2ff37fcb88ae47b82c7079feb&src=http://s7.sinaimg.cn/mw690/006BKUGwzy75VK46FMi66&690)\n",
    "\n",
    "> \n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一部分，需要各位同学首先定义自己的语言。 大家可以先想一个应用场景，然后在这个场景下，定义语法。例如：\n",
    "\n",
    "在西部世界里，一个”人类“的语言可以定义为：\n",
    "``` \n",
    "human = \"\"\"\n",
    "human = 自己 寻找 活动\n",
    "自己 = 我 | 俺 | 我们 \n",
    "寻找 = 看看 | 找找 | 想找点\n",
    "活动 = 乐子 | 玩的\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "一个“接待员”的语言可以定义为\n",
    "```\n",
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请定义你自己的语法: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "waiter = \"\"\"\n",
    "waiter => \"\"\n",
    "waiter => 问候 迎接 询问 需求 物品 结尾\n",
    "问候 => 称呼 打招呼 | 打招呼\n",
    "称呼 => 先生 | 女生 | 老人家 | 小朋友\n",
    "打招呼 => 下午好 | 晚上好 | 早上好 | 你好 | 您好\n",
    "迎接 => 欢迎光临\n",
    "询问 => 请问您需要\n",
    "需求 => 吃 | 喝 | 玩 | 看 | 听\n",
    "物品 => 点什么\n",
    "结尾 => 吗？\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sporter = \"\"\"\n",
    "sporter => \"\"\n",
    "sporter => 运动 头衔 姓名 时间 动作 赛事 等级 谓语 头衔\n",
    "运动 => 著名 羽毛球\n",
    "头衔 => 运动员 世界冠军 | 伟大的运动员 | 四大天王\n",
    "姓名 => 李宗伟 | 林丹 | 陶菲克 | 盖德\n",
    "时间 => 曾经 | 过去 | 以前 | 很早就\n",
    "动作 => 获得 | 失去 | 卫冕 | 夺得 \n",
    "赛事 => 世锦赛 | 马来西亚公开赛 | 奥运会 | 苏迪曼杯 | 汤姆斯杯 | 亚运会 | 全英公开赛\n",
    "等级 => 冠军 | 亚军 | 季军 \n",
    "谓语 => 是\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "human = \"\"\"\n",
    "human = 自己 寻找 活动\n",
    "自己 = 我 | 俺 | 我们 \n",
    "寻找 = 看看 | 找找 | 想找点\n",
    "活动 = 乐子 | 玩的\n",
    "\"\"\"\n",
    "\n",
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，使用自己之前定义的generate函数，使用此函数生成句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# step 1 解析语法\n",
    "def analytical_gram(gramStr, sp) : \n",
    "    grammer = {}\n",
    "    for line in gramStr.split('\\n') : \n",
    "        if not line.strip() : continue\n",
    "        exp, stmt = line.split(sp)\n",
    "        grammer[exp.strip()] = [s.split() for s in stmt.split('|')]\n",
    "        \n",
    "    return grammer\n",
    "    \n",
    "# step 2 根据语法生成\n",
    "def generate(gram, target) : \n",
    "    if target not in gram : return target\n",
    "    new_expanded = random.choice(gram[target])\n",
    "    return ''.join(generate(gram,  t) for t in new_expanded if t != 'null') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'先生晚上好欢迎光临请问您需要听点什么吗？'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waiter_gram = analytical_gram(waiter, '=>')\n",
    "generate(waiter_gram, 'waiter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'著名羽毛球伟大的运动员盖德很早就卫冕亚运会季军是四大天王'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sproter_gram = analytical_gram(sporter, '=>')\n",
    "generate(sproter_gram, 'sporter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，定义一个函数，generate_n，将generate扩展，使其能够生成n个句子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n(n):\n",
    "    # you code here \n",
    "    grams = {\"waiter\" : analytical_gram(waiter, '=>'),  \n",
    "                 \"sporter\" : analytical_gram(sporter, '=>'),\n",
    "#                  \"human\":analytical_gram(human, '='),\n",
    "                 \"host\":analytical_gram(host, '=')}\n",
    "    keys = [key for key in grams.keys()]\n",
    "    \n",
    "    stmts = []\n",
    "    for i in range(0, n) : \n",
    "        key = random.choice(keys)\n",
    "        stmts.append(generate(grams[key], key))\n",
    "        \n",
    "    return stmts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['小朋友早上好欢迎光临请问您需要看点什么吗？',\n",
       " '女生下午好欢迎光临请问您需要吃点什么吗？',\n",
       " '晚上好欢迎光临请问您需要玩点什么吗？',\n",
       " '您好我是9号,您需要耍一耍打猎吗？',\n",
       " '你好我是379号,您需要耍一耍喝酒吗？',\n",
       " '著名羽毛球伟大的运动员盖德过去夺得苏迪曼杯冠军是伟大的运动员',\n",
       " '您好欢迎光临请问您需要听点什么吗？',\n",
       " '老人家早上好欢迎光临请问您需要喝点什么吗？',\n",
       " '著名羽毛球伟大的运动员李宗伟以前卫冕全英公开赛亚军是伟大的运动员',\n",
       " '您好我是4号,您需要耍一耍打猎吗？']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_n(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 使用新数据源完成语言模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照我们上文中定义的`prob_2`函数，我们更换一个文本数据源，获得新的Language Model:\n",
    "\n",
    "1. 下载文本数据集（你可以在以下数据集中任选一个，也可以两个都使用）\n",
    "    + 可选数据集1，保险行业问询对话集： https://github.com/Computing-Intelligence/insuranceqa-corpus-zh/raw/release/corpus/pool/train.txt.gz\n",
    "    + 可选数据集2：豆瓣评论数据集：https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv\n",
    "2. 修改代码，获得新的**2-gram**语言模型\n",
    "    + 进行文本清洗，获得所有的纯文本\n",
    "    + 将这些文本进行切词\n",
    "    + 送入之前定义的语言模型中，判断文本的合理程度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本清理\n",
    "import os\n",
    "import re\n",
    "import jieba\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# 新闻数据\n",
    "news_file = '/Users/yeah/Downloads/sqlResult_1558435.csv'\n",
    "# 电影评论数据\n",
    "movies_file =  '/Users/yeah/Downloads/movie_comments.csv'\n",
    "# 保险评论数据\n",
    "insurance_file = '/Users/yeah/Downloads/train.txt'\n",
    "# 分词字典\n",
    "dicts_file = './dict.json'\n",
    "\n",
    "# 获取文本内容\n",
    "def token(string):\n",
    "    return re.findall('\\w+', string)\n",
    "\n",
    "# jieba分词\n",
    "def cut(string):\n",
    "    return list(jieba.cut(string))\n",
    "\n",
    "# 分词处理文本文件\n",
    "def cut_all(file):\n",
    "    cuts = []\n",
    "    for i, line in enumerate(open(file).readline()):\n",
    "        # print(\"cut line num %d\" % i)\n",
    "        # if i == 100:\n",
    "        #     break\n",
    "        cuts.append(cut(line))\n",
    "    return cuts\n",
    "\n",
    "# 读取分词字典\n",
    "def load_dict(file):\n",
    "    word_dicts = {}\n",
    "    if os.path.exists(file):\n",
    "        with open(file, 'r') as f:\n",
    "            word_dicts = json.loads(f.read())\n",
    "    return word_dicts\n",
    "\n",
    "# 读取csv\n",
    "def load_csv(file, key, encode):\n",
    "    content = pd.read_csv(file, encoding=encode)\n",
    "    return content[key].tolist()\n",
    "\n",
    "# 读取txt\n",
    "# def load_txt(file) :\n",
    "#     with open(file, 'r') as f :\n",
    "#         word_dicts = json.loads(f.read())\n",
    "\n",
    "# 清洗数据  去除无效字符 生成分词字典\n",
    "def build_dicts(contents, key):\n",
    "    cache_file = './cache.txt'\n",
    "    now = time.time()\n",
    "    #  清理数据写入缓存文件\n",
    "    for a in contents:\n",
    "        with open(cache_file, 'a') as f:\n",
    "            f.write(''.join(token(str(a))))\n",
    "    end = time.time()\n",
    "    print(\"cache file spent %d\" % (end - now))\n",
    "\n",
    "    #  分词\n",
    "    now = time.time()\n",
    "    dicts = load_dict(dicts_file)\n",
    "    if key in dicts:\n",
    "        print(\"before cut, dicts[%s] length : %d\" % (key, len(dicts[key])))\n",
    "    else:\n",
    "        print(\"before cut, dicts[%s] length : %d\" % (key, 0))\n",
    "\n",
    "    cuts = cut_all(cache_file)\n",
    "    end = time.time()\n",
    "    print(\"cut spent %d, cuts length : %d\" % (end - now, len(cuts)))\n",
    "\n",
    "    # dicts[key] = reduce(add, cuts) reduce 太慢了，先写到文件，后面用正则替换掉'[]'\n",
    "    dicts[key] = re.findall('[^\\[\\],\\s\\\"\\']', str(cuts))\n",
    "    print(\"after cut, dicts[%s] length : %d\" % (key, len(dicts[key])))\n",
    "\n",
    "    # 保存文件\n",
    "    with open(dicts_file, 'w') as f:\n",
    "        f.write(json.dumps(dicts))\n",
    "\n",
    "    # 文本处理完后删除缓存文件\n",
    "    os.remove(cache_file)\n",
    "\n",
    "def init_dicts() :\n",
    "    dicts = load_dict(dicts_file)\n",
    "    TOKENS = dicts['news'] + dicts['movies']\n",
    "    TOKENS_COUNT = Counter(TOKENS)\n",
    "\n",
    "    TOKENS2 = [\n",
    "        ''.join(TOKENS[i:i + 2]) for i in range(len(TOKENS[:-2]))\n",
    "    ]\n",
    "    TOKENS_COUNT2 = Counter(TOKENS2)\n",
    "    \n",
    "    return TOKENS, TOKENS_COUNT, TOKENS2, TOKENS_COUNT2\n",
    "\n",
    "def init_dicts_n(n) : \n",
    "    TOKENS_N = [\n",
    "        ''.join(TOKENS[i:i + n]) for i in range(len(TOKENS[:-n]))\n",
    "    ]\n",
    "    TOKENS_COUNT_N = Counter(TOKENS_N)\n",
    "    \n",
    "    return TOKENS_N, TOKENS_COUNT_N\n",
    "\n",
    "def prob_1(w):\n",
    "    return TOKENS_COUNT[w] / len(TOKENS)\n",
    "\n",
    "def prob_2(w1, w2):\n",
    "    if w1 + w2 in TOKENS_COUNT2:\n",
    "        return TOKENS_COUNT2[w1 + w2] / len(TOKENS2)\n",
    "    else:\n",
    "        return 1 / len(TOKENS2)\n",
    "    \n",
    "def prob_n(ws):\n",
    "    if \"\".join(ws) in TOKENS_N:\n",
    "        return TOKENS_COUNT_N[\"\".join(ws)] / len(TOKENS_N)\n",
    "    else:\n",
    "        return 1 / len(TOKENS_N)    \n",
    "\n",
    "def get_probablity(sentence):\n",
    "    words = cut(sentence)\n",
    "    sen_pro = 1\n",
    "    for i, word in enumerate(sentence[:-1]):\n",
    "        next_ = sentence[i + 1]\n",
    "        pro = prob_2(word, next_)\n",
    "        sen_pro *= pro\n",
    "    print('2_gram pro(%s) : %g' % (sentence, sen_pro))\n",
    "    return sen_pro\n",
    "\n",
    "def get_probablity_n(sentence, n):\n",
    "    words = cut(sentence)\n",
    "    sen_pro = 1\n",
    "    for i, _word in enumerate(sentence[:-1]):\n",
    "        pro = prob_n(sentence[i:i + n])\n",
    "        sen_pro *= pro\n",
    "    print('%d_gram pro(%s) : %g' % (n, sentence, sen_pro))\n",
    "    return sen_pro\n",
    "\n",
    "# build_dicts(load_csv(news_file, 'content', 'gb18030'), 'news')\n",
    "# build_dicts(load_csv(movies_file, 'comment', 'utf8'), 'movies')\n",
    "\n",
    "def test() : \n",
    "    waiter_gram = analytical_gram(waiter, '=>')\n",
    "    s1 = generate(waiter_gram, 'waiter')\n",
    "    get_probablity(s1)\n",
    "    \n",
    "    sproter_gram = analytical_gram(sporter, '=>')\n",
    "    s2 = generate(sproter_gram, 'sporter')\n",
    "    get_probablity(s2)\n",
    "    \n",
    "    human_gram = analytical_gram(human, '=')\n",
    "    s3 = generate(human_gram, 'human')\n",
    "    get_probablity(s3)\n",
    "        \n",
    "    host_gram = analytical_gram(host, '=')\n",
    "    s4 = generate(host_gram, 'host')\n",
    "    get_probablity(s4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS, TOKENS_COUNT, TOKENS2, TOKENS_COUNT2 = init_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS_N, TOKENS_COUNT_N = init_dicts_n(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41343590"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TOKENS_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('新华', 151622),\n",
       " ('华社', 144487),\n",
       " ('20', 142995),\n",
       " ('nn', 137285),\n",
       " ('01', 117875),\n",
       " ('17', 92129),\n",
       " ('中国', 91260),\n",
       " ('n新', 89128),\n",
       " ('外代', 83291),\n",
       " ('7年', 66379)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENS_COUNT2.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 1056962),\n",
       " ('n', 707762),\n",
       " ('一', 406498),\n",
       " ('国', 357916),\n",
       " ('在', 351681),\n",
       " ('1', 334522),\n",
       " ('是', 311231),\n",
       " ('中', 305236),\n",
       " ('2', 289652),\n",
       " ('0', 285249)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENS_COUNT.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2017年', 64512),\n",
       " ('新华社照片', 50401),\n",
       " ('新华社记者', 30866),\n",
       " ('n新华社记', 25854),\n",
       " ('017年5', 24550),\n",
       " ('17年5月', 24550),\n",
       " ('17年4月', 22928),\n",
       " ('017年4', 22925),\n",
       " ('n外代二线', 21257),\n",
       " ('日n外代二', 20847)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENS_COUNT_N.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41343593"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TOKENS2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_gram pro(你好我是8号,您需要玩一玩赌博吗？) : 6.27244e-98\n",
      "5_gram pro(女士,你好我是45号,请问你要玩一玩赌博吗？) : 1.13613e-160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.13613022018765e-160"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probablity(generate(analytical_gram(host, '='), 'host'))\n",
    "get_probablity_n(generate(analytical_gram(host, '='), 'host'), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_gram pro(今天真热啊) : 5.45895e-24\n",
      "5_gram pro(今天真热啊) : 3.42269e-31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.4226875838662093e-31"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probablity(\"今天真热啊\")\n",
    "get_probablity_n(\"今天真热啊\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pro(先生下午好欢迎光临请问您需要听点什么吗？) : 1.62288e-109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.6228840481425581e-109"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probablity(generate(analytical_gram(waiter, '=>'), 'waiter'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pro(我们看看玩的) : 1.79304e-24\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.793041472542345e-24"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probablity(generate(analytical_gram(human, '='), 'human'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pro(著名羽毛球伟大的运动员林丹曾经夺得苏迪曼杯季军是四大天王) : 5.08418e-139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.084178124948493e-139"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probablity(generate(analytical_gram(sporter, '=>'), 'sporter'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pro(您好欢迎光临请问您需要听点什么吗？) : 1.9083e-95\n",
      "pro(著名羽毛球伟大的运动员李宗伟过去卫冕马来西亚公开赛亚军是四大天王) : 8.00543e-156\n",
      "pro(我们想找点玩的) : 4.43724e-33\n",
      "pro(您好我是6号,您需要耍一耍喝酒吗？) : 5.30407e-103\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pro(新盖中盖，一口气上六楼不带喘！) : 6.23431e-91\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.234306588461713e-91"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probablity(\"新盖中盖，一口气上六楼不带喘！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 获得最优质的的语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们能够生成随机的语言并且能判断之后，我们就可以生成更加合理的语言了。请定义 generate_best 函数，该函数输入一个语法 + 语言模型，能够生成**n**个句子，并能选择一个最合理的句子: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示，要实现这个函数，你需要Python的sorted函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 5]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([1, 3, 5, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数接受一个参数key，这个参数接受一个函数作为输入，例如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (2, 5), (4, 4), (5, 0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第0个元素进行排序."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0), (1, 4), (4, 4), (2, 5)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第1个元素进行排序."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 5), (1, 4), (4, 4), (5, 0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第1个元素进行排序, 但是是递减的顺序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_best(): # you code here\n",
    "    stmts = [(s, get_probablity(s)) for s in generate_n(10000)]\n",
    "    sorted_stmts = sorted(stmts, key=lambda x: x[1],  reverse=True)\n",
    "    return sorted_stmts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('你好欢迎光临请问您需要看点什么吗？', 2.439190457125717e-91)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，现在我们实现了自己的第一个AI模型，这个模型能够生成比较接近于人类的语言。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 这个模型有什么问题？ 你准备如何提升？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 样本空间还不够，计算频率偏低，语言准确性判断太低；可以增加分词库，分词库需要清理一些不必要的数字；按理解来说N_gram中N越大判断越精确，尝试改变N的大小测试下; 测试N增大似乎对准确率并没有明显提升,可能词库太单一或者太少？！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 以下内容为可选部分，对于绝大多数同学，能完成以上的项目已经很优秀了，下边的内容如果你还有精力可以试试，但不是必须的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. (Optional) 完成基于Pattern Match的语句问答\n",
    "> 我们的GitHub仓库中，有一个assignment-01-optional-pattern-match，这个难度较大，感兴趣的同学可以挑战一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5. (Optional) 完成阿兰图灵机器智能原始论文的阅读\n",
    "1. 请阅读阿兰图灵关于机器智能的原始论文：https://github.com/Computing-Intelligence/References/blob/master/AI%20%26%20Machine%20Learning/Computer%20Machinery%20and%20Intelligence.pdf \n",
    "2. 并按照GitHub仓库中的论文阅读模板，填写完毕后发送给我: mqgao@kaikeba.com 谢谢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各位同学，我们已经完成了自己的第一个AI模型，大家对人工智能可能已经有了一些感觉，人工智能的核心就是，我们如何设计一个模型、程序，在外部的输入变化的时候，我们的程序不变，依然能够解决问题。人工智能是一个很大的领域，目前大家所熟知的深度学习只是其中一小部分，之后也肯定会有更多的方法提出来，但是大家知道人工智能的目标，就知道了之后进步的方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，希望大家对AI不要有恐惧感，这个并不难，大家加油！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1561828422005&di=48d19c16afb6acc9180183a6116088ac&imgtype=0&src=http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201807%2F28%2F20180728150843_BECNF.thumb.224_0.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
